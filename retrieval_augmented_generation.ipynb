{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidracodes/API/blob/main/retrieval_augmented_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-nx7tB9eoM7"
      },
      "source": [
        "## Retrieval Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yQ1fK2SeoM9"
      },
      "source": [
        "Retrieval Augmented Generation (RAG) [[1](https://arxiv.org/abs/2005.11401v4)] is an advanced NLP technique that enhances the quality and reliability of Large Language Models (LLMs) by grounding them in external knowledge sources.\n",
        "\n",
        "In practice, this approach combines information retrieval with text generation as follows:\n",
        "1. Given a user query (prompt), the system accesses an external large knowledge base (such as a vector index) to find relevant passages.\n",
        "2. It then augments the original query with this retrieved information.\n",
        "3. The LLM generates a response based on both the original query and the augmented context.\n",
        "\n",
        "Key benefits of implementing RAG in LLM-based systems include:\n",
        "1. More factual and specific response generation.\n",
        "2. Easy incorporation of updated knowledge by modifying the retrieval corpus without retraining the LLM.\n",
        "3. Provides a form of interpretability by citing the retrieved passages used for generation.\n",
        "\n",
        "[1] Lewis P, et al. 2020. *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*. [arXiv:2005.11401](https://arxiv.org/abs/2005.11401v4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gk6BTsteoM-"
      },
      "source": [
        "In this notebook, we'll build a basic knowledge base with exemplary documents, apply chunking, index the embedded splits into a vector storage, and build a conversational chain with history:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/dcarpintero/generative-ai-101/blob/main/static/retrieval_augmented_generation.png?raw=1\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BB2I03oeoM-"
      },
      "source": [
        "### 1. Build up Knowledge Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZfCXGDAeoM_"
      },
      "source": [
        "The most common approach in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
        "\n",
        "In this basic example, we will take two sources related to the Llama 3.1 model, split them into chunks, embed them using an open-source embedding model, and load them into a vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GepDV0-GeoM_",
        "outputId": "e95879f8-ad06-4128-d265-5181c2f89325",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.5/49.5 kB 1.8 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain langchain-community langchain-huggingface sentence-transformers faiss-cpu bs4 --quiet | tail -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g1cJreweoNB"
      },
      "source": [
        "#### 1.1 Document Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIv3tbszeoNB"
      },
      "source": [
        "We first load the document(s) from web url's:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyqFv0DxeoNB"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\"https://ai.meta.com/blog/meta-llama-3-1/\",\n",
        "                        \"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md\"])\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7yXF8I2eoNB"
      },
      "source": [
        "#### 1.2 Chunking Documents for RAG\n",
        "\n",
        "A critical step in implementing Retrieval-Augmented Generation (RAG) is splitting documents into appropriate chunks. This process ensures that semantically relevant content is grouped together, optimizing retrieval accuracy and context preservation. In this section we will explore how to effectively chunk our documents using LangChain.\n",
        "\n",
        "##### Why Chunking Matters\n",
        "\n",
        "1. **Semantic Coherence**: Proper chunking keeps related information together, improving the relevance of retrieved content.\n",
        "2. **Context Window Optimization**: Chunks should fit within the LLM's context window for efficient processing.\n",
        "3. **Retrieval Precision**: Well-defined chunks enable more accurate and targeted information retrieval.\n",
        "\n",
        "##### Using LangChain's Text Splitters\n",
        "\n",
        "LangChain offers various text splitters, with the `RecursiveCharacterTextSplitter` being a recommended choice for generic text. This splitter is intended to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdipHh2ueoNC"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44RW-z2seoNC"
      },
      "source": [
        "Let's inspect the second and third chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfbbxd02eoNC"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def md(s):\n",
        "    display(Markdown(s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhgJoET_eoNC",
        "outputId": "d82844b9-fd0a-4cc2-f90f-f2fad0b002e4"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Our approachResearchProduct experiencesLlamaBlogTry Meta AILarge Language ModelIntroducing Llama 3.1: Our most capable models to dateJuly 23, 2024•15 minute readTakeaways:Meta is committed to openly accessible AI. Read Mark Zuckerberg’s letter detailing why open source is good for developers, good for Meta, and good for the world.Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first frontier-level open source AI model.Llama 3.1 405B is in a class of its own, with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models. Our new model will enable the community to unlock new workflows, such as synthetic data generation and model distillation.We’re continuing to build out Llama to be a system by providing more components that work"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "md(splits[1].page_content)\n",
        "md(splits[2].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_y_JoRMeoNC"
      },
      "source": [
        "We can see that there is indeed an overlap among those chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXQ-Q7QceoND",
        "outputId": "9bbf230c-6636-4563-e6c6-61fc7fa460c9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "and context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first fro"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "md(splits[1].page_content[-100:])\n",
        "md(splits[2].page_content[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCNqW667eoND"
      },
      "source": [
        "You might also experiment with chunking strategies at https://chunkviz.up.railway.app/, a tool that highlights splits and overlaps for common splitters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hatrqfj8eoND"
      },
      "source": [
        "![RAG Chunking](https://github.com/dcarpintero/generative-ai-101/blob/main/static/rag_chunking.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JftYABuCeoND"
      },
      "source": [
        "#### 1.3 Embedding Transformation & Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27K6F3ieoND"
      },
      "source": [
        "Let's load the documents into a vector storage with an open-source embedding model. In this example we use [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/), which is highly optimized for large-scale datasets and GPU acceleration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC_qm3G9eoND"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "db = FAISS.from_documents(documents = splits,\n",
        "                          embedding = HuggingFaceEmbeddings(model_name=embedding_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqagMCEueoNE"
      },
      "source": [
        "### 2. Foundation Models on Groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKlGfR56eoNE"
      },
      "source": [
        "You might get a GROQ API KEY at https://console.groq.com/keys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhgdCadQeoNE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "GROQ_API_TOKEN = getpass()\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJp7h4cZeoNE"
      },
      "source": [
        "In this example we will use [Llama3-8b](https://ai.meta.com/blog/meta-llama-3-1/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNxXG_xieoNE"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWBQzewqeoNE"
      },
      "source": [
        "### 3. Generate a Retrieval-Augmented Response with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6ErgwMHeoNE"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "chat_history = []\n",
        "chain = ConversationalRetrievalChain.from_llm(llm,\n",
        "                                              db.as_retriever(),\n",
        "                                              return_source_documents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdXoQvqVeoNF"
      },
      "source": [
        "We ask a very specific question about LLama 3.1, namely the size of the context length in Llama 3.1, the LLM generated response should be '128k':"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJE2m9iseoNF"
      },
      "source": [
        "![RAG Source](https://github.com/dcarpintero/generative-ai-101/blob/main/static/rag_source.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQxLlPMoeoNF"
      },
      "source": [
        "##### 3.1 Model Inference with RAG & Source Citation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1I0oQHteoNF",
        "outputId": "663bcd63-4075-41c6-efb1-a0a92e86f9fa"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "According to the text, the context length in Llama 3.1 405B is 128K."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "user_query = \"how long is the context length in Llama 3.1 405B?\"\n",
        "llm_output = chain.invoke({\"question\": user_query, \"chat_history\": chat_history})\n",
        "\n",
        "md(llm_output['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSspwAYGeoNG"
      },
      "source": [
        "LangChain includes the sources in the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7o9ScC6eoNG",
        "outputId": "d35fcfd8-0dd0-42b9-cebc-5560d66acd19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://ai.meta.com/blog/meta-llama-3-1/', 'title': 'Introducing Llama 3.1: Our most capable models to date', 'description': 'Bringing open intelligence to all, our latest models expand context length, add support across eight languages, and include Meta Llama 3.1 405B— the...', 'language': 'en'}, page_content='Our approachResearchProduct experiencesLlamaBlogTry Meta AILarge Language ModelIntroducing Llama 3.1: Our most capable models to dateJuly 23, 2024•15 minute readTakeaways:Meta is committed to openly accessible AI. Read Mark Zuckerberg’s letter detailing why open source is good for developers, good for Meta, and good for the world.Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first'),\n",
              " Document(metadata={'source': 'https://ai.meta.com/blog/meta-llama-3-1/', 'title': 'Introducing Llama 3.1: Our most capable models to date', 'description': 'Bringing open intelligence to all, our latest models expand context length, add support across eight languages, and include Meta Llama 3.1 405B— the...', 'language': 'en'}, page_content='context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first frontier-level open source AI model.Llama 3.1 405B is in a class of its own, with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models. Our new model will enable the community to unlock new workflows, such as synthetic data generation and model distillation.We’re continuing to build out Llama to be a system by providing more components that work'),\n",
              " Document(metadata={'source': 'https://ai.meta.com/blog/meta-llama-3-1/', 'title': 'Introducing Llama 3.1: Our most capable models to date', 'description': 'Bringing open intelligence to all, our latest models expand context length, add support across eight languages, and include Meta Llama 3.1 405B— the...', 'language': 'en'}, page_content='math or coding question.RECOMMENDED READSExpanding the Llama ecosystem responsiblyThe Llama ecosystem: Past, present, and futureUntil today, open source large language models have mostly trailed behind their closed counterparts when it comes to capabilities and performance. Now, we’re ushering in a new era with open source leading the way. We’re publicly releasing Meta Llama 3.1 405B, which we believe is the world’s largest and most capable openly available foundation model. With more than 300'),\n",
              " Document(metadata={'source': 'https://ai.meta.com/blog/meta-llama-3-1/', 'title': 'Introducing Llama 3.1: Our most capable models to date', 'description': 'Bringing open intelligence to all, our latest models expand context length, add support across eight languages, and include Meta Llama 3.1 405B— the...', 'language': 'en'}, page_content='a request for comment on the Llama Stack API, a standard interface we hope will make it easier for third-party projects to leverage Llama models.The ecosystem is primed and ready to go with over 25 partners, including AWS, NVIDIA, Databricks, Groq, Dell, Azure, Google Cloud, and Snowflake offering services on day one.Try Llama 3.1 405B in the US on WhatsApp and at meta.ai by asking a challenging math or coding question.RECOMMENDED READSExpanding the Llama ecosystem responsiblyThe Llama')]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_output['source_documents']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSZHUMyueoNG"
      },
      "source": [
        "We can see that the first source includes indeed the answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oKjBxNueoNG",
        "outputId": "71847eb8-7d90-4bd5-979e-172935bca5b7"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Our approachResearchProduct experiencesLlamaBlogTry Meta AILarge Language ModelIntroducing Llama 3.1: Our most capable models to dateJuly 23, 2024•15 minute readTakeaways:Meta is committed to openly accessible AI. Read Mark Zuckerberg’s letter detailing why open source is good for developers, good for Meta, and good for the world.Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "md(llm_output['source_documents'][0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndGdeejheoNG"
      },
      "source": [
        "##### 3.2 Follow-up Question with Chat History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl9pvMKVeoNH"
      },
      "outputs": [],
      "source": [
        "chat_history = [(user_query, llm_output[\"answer\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr_jw-EkeoNH"
      },
      "source": [
        "Including the chat history allows the the model to correctly infer the intent, namely that the user is asking about the context length of the '8b model':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaJRJbEaeoNI",
        "outputId": "9bb888b2-59e1-4da3-8394-2d178c32a092"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "According to the text, the context length of the 8B model is 128K."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "user_query = \"what about the 8b model?\"\n",
        "llm_output = chain.invoke({\"question\": user_query, \"chat_history\": chat_history})\n",
        "md(llm_output['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6U8g-UjeoNJ"
      },
      "source": [
        "##### 3.3 Same Question without Chat History is Not Accurate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk4eZ5SoeoNK",
        "outputId": "4008277e-ac9c-486a-ce85-bf4d4ca9c5ff"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The text does not mention the \"8b model\". It does mention quantizing the 405B model from 16-bit (BF16) to 8-bit (FP8) numerics, but it does not mention an \"8b model\" specifically."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "user_query = \"what about the 8b model?\"\n",
        "llm_output = chain.invoke({\"question\": user_query, \"chat_history\": []})\n",
        "md(llm_output['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNZq5ZGbeoNK"
      },
      "source": [
        "Without chat history, the model appears to just retrieve passages that approximate the semantic meaning of the word 'model' contained in the user question, but is not able to retrieve information about the context length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RFzz8qaeoNK",
        "outputId": "b1931049-bb89-48d3-dd24-5a6d10a1c9ef"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Introducing Llama 3.1: Our most capable models to date"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "this blog post.)While this is our biggest model yet, we believe there’s still plenty of new ground to explore in the future, including more device-friendly sizes, additional modalities, and more investment at the agent platform layer.As always, we look forward to seeing all the amazing products and experiences the community will build with these models.This work was supported by our partners across the AI community. We’d like to thank and acknowledge (in alphabetical order): Accenture, Amazon"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "parameter model to improve the post-training quality of our smaller models.To support large-scale production inference for a model at the scale of the 405B, we quantized our models from 16-bit (BF16) to 8-bit (FP8) numerics, effectively lowering the compute requirements needed and allowing the model to run within a single server node.Instruction and chat fine-tuningWith Llama 3.1 405B, we strove to improve the helpfulness, quality, and detailed instruction-following capability of the model in"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "translation. With the release of the 405B model, we’re poised to supercharge innovation—with unprecedented opportunities for growth and exploration. We believe the latest generation of Llama will ignite new applications and modeling paradigms, including synthetic data generation to enable the improvement and training of smaller models, as well as model distillation—a capability that has never been achieved at this scale in open source.As part of this latest release, we’re introducing upgraded"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for doc in llm_output['source_documents']:\n",
        "    md(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFMp8hFEeoNL"
      },
      "source": [
        "### 4. Model Hallucination without RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0GNOH-JeoNL"
      },
      "source": [
        "Note that without RAG, the model generates an incorrect response, and that the user can not verify the information since the sources are not available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XErAxQGFeoNL",
        "outputId": "159d1797-a747-43af-db50-4530edad9bdc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "According to the official documentation, the context length in LLaMA 3.1 405B is 2048 tokens.\n",
              "\n",
              "In other words, the model can process and respond to input sequences of up to 2048 tokens (or characters) in length."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "result = llm.invoke(\"how long is the context length in Llama 3.1 405B?\")\n",
        "md(result.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}