{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-nx7tB9eoM7"
      },
      "source": [
        "## Retrieval Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yQ1fK2SeoM9"
      },
      "source": [
        "Retrieval Augmented Generation (RAG) [[1](https://arxiv.org/abs/2005.11401v4)] is an advanced NLP technique that enhances the quality and reliability of Large Language Models (LLMs) by grounding them in external knowledge sources.\n",
        "\n",
        "In practice, this approach combines information retrieval with text generation as follows:\n",
        "1. Given a user query (prompt), the system accesses an external large knowledge base (such as a vector index) to find relevant passages.\n",
        "2. It then augments the original query with this retrieved information.\n",
        "3. The LLM generates a response based on both the original query and the augmented context.\n",
        "\n",
        "Key benefits of implementing RAG in LLM-based systems include:\n",
        "1. More factual and specific response generation.\n",
        "2. Easy incorporation of updated knowledge by modifying the retrieval corpus without retraining the LLM.\n",
        "3. Provides a form of interpretability by citing the retrieved passages used for generation.\n",
        "\n",
        "[1] Lewis P, et al. 2020. *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*. [arXiv:2005.11401](https://arxiv.org/abs/2005.11401v4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gk6BTsteoM-"
      },
      "source": [
        "In this notebook, we'll build a basic knowledge base with exemplary documents, apply chunking, index the embedded splits into a vector storage, and build a conversational chain with history:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/dcarpintero/generative-ai-101/blob/main/static/retrieval_augmented_generation.png?raw=1\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BB2I03oeoM-"
      },
      "source": [
        "### 1. Build up Knowledge Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZfCXGDAeoM_"
      },
      "source": [
        "The most common approach in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
        "\n",
        "In this basic example, we will take two sources related to the Llama 3.1 model, split them into chunks, embed them using an open-source embedding model, and load them into a vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GepDV0-GeoM_",
        "outputId": "e95879f8-ad06-4128-d265-5181c2f89325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.5/49.5 kB 1.8 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain langchain-community langchain-huggingface sentence-transformers faiss-cpu bs4 --quiet | tail -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g1cJreweoNB"
      },
      "source": [
        "#### 1.1 Document Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIv3tbszeoNB"
      },
      "source": [
        "We first load the document(s) from web url's:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyqFv0DxeoNB",
        "outputId": "a726f90e-48e1-444c-a0e7-3b88b4276f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\"https://ai.meta.com/blog/meta-llama-3-1/\",\n",
        "                        \"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md\"])\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7yXF8I2eoNB"
      },
      "source": [
        "#### 1.2 Chunking Documents for RAG\n",
        "\n",
        "A critical step in implementing Retrieval-Augmented Generation (RAG) is splitting documents into appropriate chunks. This process ensures that semantically relevant content is grouped together, optimizing retrieval accuracy and context preservation. In this section we will explore how to effectively chunk our documents using LangChain.\n",
        "\n",
        "##### Why Chunking Matters\n",
        "\n",
        "1. **Semantic Coherence**: Proper chunking keeps related information together, improving the relevance of retrieved content.\n",
        "2. **Context Window Optimization**: Chunks should fit within the LLM's context window for efficient processing.\n",
        "3. **Retrieval Precision**: Well-defined chunks enable more accurate and targeted information retrieval.\n",
        "\n",
        "##### Using LangChain's Text Splitters\n",
        "\n",
        "LangChain offers various text splitters, with the `RecursiveCharacterTextSplitter` being a recommended choice for generic text. This splitter is intended to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hdipHh2ueoNC"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44RW-z2seoNC"
      },
      "source": [
        "Let's inspect the second and third chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mfbbxd02eoNC"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def md(s):\n",
        "    display(Markdown(s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "PhgJoET_eoNC",
        "outputId": "be52e41c-fb87-4922-f851-eb27a5332504"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Our approachResearchProduct experiencesLlamaBlogTry Meta AILarge Language ModelIntroducing Llama 3.1: Our most capable models to dateJuly 23, 2024•15 minute readTakeaways:Meta is committed to openly accessible AI. Read Mark Zuckerberg’s letter detailing why open source is good for developers, good for Meta, and good for the world.Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first frontier-level open source AI model.Llama 3.1 405B is in a class of its own, with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models. Our new model will enable the community to unlock new workflows, such as synthetic data generation and model distillation.We’re continuing to build out Llama to be a system by providing more components that work"
          },
          "metadata": {}
        }
      ],
      "source": [
        "md(splits[1].page_content)\n",
        "md(splits[2].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_y_JoRMeoNC"
      },
      "source": [
        "We can see that there is indeed an overlap among those chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "PXQ-Q7QceoND",
        "outputId": "67819d5d-a372-430a-83be-ba1487482959"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "and context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first fro"
          },
          "metadata": {}
        }
      ],
      "source": [
        "md(splits[1].page_content[-100:])\n",
        "md(splits[2].page_content[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCNqW667eoND"
      },
      "source": [
        "You might also experiment with chunking strategies at https://chunkviz.up.railway.app/, a tool that highlights splits and overlaps for common splitters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hatrqfj8eoND"
      },
      "source": [
        "![RAG Chunking](https://github.com/dcarpintero/generative-ai-101/blob/main/static/rag_chunking.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JftYABuCeoND"
      },
      "source": [
        "#### 1.3 Embedding Transformation & Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27K6F3ieoND"
      },
      "source": [
        "Let's load the documents into a vector storage with an open-source embedding model. In this example we use [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/), which is highly optimized for large-scale datasets and GPU acceleration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uC_qm3G9eoND"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "db = FAISS.from_documents(documents = splits,\n",
        "                          embedding = HuggingFaceEmbeddings(model_name=embedding_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqagMCEueoNE"
      },
      "source": [
        "### 2. Foundation Models on Groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKlGfR56eoNE"
      },
      "source": [
        "You might get a GROQ API KEY at https://console.groq.com/keys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhgdCadQeoNE",
        "outputId": "428538d6-ca58-412c-8368-b7a540a8c2b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "GROQ_API_TOKEN = getpass()\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJp7h4cZeoNE"
      },
      "source": [
        "In this example we will use [Llama3-8b](https://ai.meta.com/blog/meta-llama-3-1/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNxXG_xieoNE",
        "outputId": "99f8c20d-b7a7-4f70-fe70-c914d9578c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.2.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.14.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.3.29)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.10.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-groq) (0.2.10)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-groq) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-groq) (9.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-groq) (3.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-groq) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-groq) (2.3.0)\n",
            "Downloading langchain_groq-0.2.3-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.14.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.14.0 langchain-groq-0.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-groq\n",
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWBQzewqeoNE"
      },
      "source": [
        "### 3. Generate a Retrieval-Augmented Response with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d6ErgwMHeoNE"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "chat_history = []\n",
        "chain = ConversationalRetrievalChain.from_llm(llm,\n",
        "                                              db.as_retriever(),\n",
        "                                              return_source_documents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdXoQvqVeoNF"
      },
      "source": [
        "We ask a very specific question about LLama 3.1, namely the size of the context length in Llama 3.1, the LLM generated response should be '128k':"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJE2m9iseoNF"
      },
      "source": [
        "![RAG Source](https://github.com/dcarpintero/generative-ai-101/blob/main/static/rag_source.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQxLlPMoeoNF"
      },
      "source": [
        "##### 3.1 Model Inference with RAG & Source Citation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "z1I0oQHteoNF",
        "outputId": "2a1ee270-c301-4941-d710-cce44591dc2c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "According to the text, the context length in Llama 3.1 405B is 128K."
          },
          "metadata": {}
        }
      ],
      "source": [
        "user_query = \"how long is the context length in Llama 3.1 405B?\"\n",
        "llm_output = chain.invoke({\"question\": user_query, \"chat_history\": chat_history})\n",
        "\n",
        "md(llm_output['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSspwAYGeoNG"
      },
      "source": [
        "LangChain includes the sources in the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7o9ScC6eoNG",
        "outputId": "901a269c-6c80-4276-81be-ae076ea9fbdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='240cde7a-9713-4fa3-99e6-558109118fd0', metadata={'source': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md', 'title': 'llama-models/models/llama3_1/MODEL_CARD.md at main · meta-llama/llama-models · GitHub', 'description': 'Utilities intended for use with Llama models. Contribute to meta-llama/llama-models development by creating an account on GitHub.', 'language': 'en'}, page_content='Language\\n\\nLlama 3.1 8B Instruct\\n\\nLlama 3.1 70B Instruct\\n\\nLlama 3.1 405B Instruct\\n\\n\\n\\nGeneral\\n\\nMMLU (5-shot, macro_avg/acc)\\n\\nPortuguese\\n   \\n62.12\\n   \\n80.13\\n   \\n84.95\\n   \\n\\n\\nSpanish\\n   \\n62.45\\n   \\n80.05\\n   \\n85.08\\n   \\n\\n\\nItalian\\n   \\n61.63\\n   \\n80.4\\n   \\n85.04\\n   \\n\\n\\nGerman\\n   \\n60.59\\n   \\n79.27\\n   \\n84.36\\n   \\n\\n\\nFrench\\n   \\n62.34\\n   \\n79.82\\n   \\n84.66\\n   \\n\\n\\nHindi\\n   \\n50.88\\n   \\n74.52\\n   \\n80.31\\n   \\n\\n\\nThai\\n   \\n50.32\\n   \\n72.95\\n   \\n78.21'),\n",
              " Document(id='8b557c4a-6419-407b-b1ac-6b88cd12279c', metadata={'source': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md', 'title': 'llama-models/models/llama3_1/MODEL_CARD.md at main · meta-llama/llama-models · GitHub', 'description': 'Utilities intended for use with Llama models. Contribute to meta-llama/llama-models development by creating an account on GitHub.', 'language': 'en'}, page_content='Metric\\n\\nLlama 3 8B Instruct\\n\\nLlama 3.1 8B Instruct\\n\\nLlama 3 70B Instruct\\n\\nLlama 3.1 70B Instruct\\n\\nLlama 3.1 405B Instruct\\n\\n\\n\\nGeneral\\n   \\nMMLU\\n   \\n5\\n   \\nmacro_avg/acc\\n   \\n68.5\\n   \\n69.4\\n   \\n82.0\\n   \\n83.6\\n   \\n87.3\\n   \\n\\n\\nMMLU (CoT)\\n   \\n0\\n   \\nmacro_avg/acc\\n   \\n65.3\\n   \\n73.0\\n   \\n80.9\\n   \\n86.0\\n   \\n88.6\\n   \\n\\n\\nMMLU-Pro (CoT)\\n   \\n5\\n   \\nmacro_avg/acc\\n   \\n45.5\\n   \\n48.3\\n   \\n63.4\\n   \\n66.4\\n   \\n73.3\\n   \\n\\n\\nIFEval\\n   \\n\\n\\n\\n\\n76.8\\n   \\n80.4\\n   \\n82.9\\n   \\n87.5\\n   \\n88.6'),\n",
              " Document(id='8489765e-b6d3-41c5-8a80-8e224577ac75', metadata={'source': 'https://ai.meta.com/blog/meta-llama-3-1/', 'title': 'Introducing Llama 3.1: Our most capable models to date', 'description': 'Bringing open intelligence to all, our latest models expand context length, add support across eight languages, and include Meta Llama 3.1 405B— the...', 'language': 'en'}, page_content='Our approachResearchProduct experiencesLlamaBlogTry Meta AILarge Language ModelIntroducing Llama 3.1: Our most capable models to dateJuly 23, 2024•15 minute readTakeaways:Meta is committed to openly accessible AI. Read Mark Zuckerberg’s letter detailing why open source is good for developers, good for Meta, and good for the world.Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first'),\n",
              " Document(id='6b0a8b0a-280a-4726-abb0-6780a58c18dd', metadata={'source': 'https://ai.meta.com/blog/meta-llama-3-1/', 'title': 'Introducing Llama 3.1: Our most capable models to date', 'description': 'Bringing open intelligence to all, our latest models expand context length, add support across eight languages, and include Meta Llama 3.1 405B— the...', 'language': 'en'}, page_content='context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first frontier-level open source AI model.Llama 3.1 405B is in a class of its own, with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models. Our new model will enable the community to unlock new workflows, such as synthetic data generation and model distillation.We’re continuing to build out Llama to be a system by providing more components that work')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "llm_output['source_documents']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSZHUMyueoNG"
      },
      "source": [
        "We can see that the first source includes indeed the answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "_oKjBxNueoNG",
        "outputId": "b462a32a-1407-4e65-cae7-271495f32b55"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Language\n\nLlama 3.1 8B Instruct\n\nLlama 3.1 70B Instruct\n\nLlama 3.1 405B Instruct\n\n\n\nGeneral\n\nMMLU (5-shot, macro_avg/acc)\n\nPortuguese\n   \n62.12\n   \n80.13\n   \n84.95\n   \n\n\nSpanish\n   \n62.45\n   \n80.05\n   \n85.08\n   \n\n\nItalian\n   \n61.63\n   \n80.4\n   \n85.04\n   \n\n\nGerman\n   \n60.59\n   \n79.27\n   \n84.36\n   \n\n\nFrench\n   \n62.34\n   \n79.82\n   \n84.66\n   \n\n\nHindi\n   \n50.88\n   \n74.52\n   \n80.31\n   \n\n\nThai\n   \n50.32\n   \n72.95\n   \n78.21"
          },
          "metadata": {}
        }
      ],
      "source": [
        "md(llm_output['source_documents'][0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndGdeejheoNG"
      },
      "source": [
        "##### 3.2 Follow-up Question with Chat History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fl9pvMKVeoNH"
      },
      "outputs": [],
      "source": [
        "chat_history = [(user_query, llm_output[\"answer\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr_jw-EkeoNH"
      },
      "source": [
        "Including the chat history allows the the model to correctly infer the intent, namely that the user is asking about the context length of the '8b model':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "KaJRJbEaeoNI",
        "outputId": "8b5335ce-922b-46bb-bd86-e505cfc3515c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "According to the text, the context length in the 8B model is 128K."
          },
          "metadata": {}
        }
      ],
      "source": [
        "user_query = \"what about the 8b model?\"\n",
        "llm_output = chain.invoke({\"question\": user_query, \"chat_history\": chat_history})\n",
        "md(llm_output['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6U8g-UjeoNJ"
      },
      "source": [
        "##### 3.3 Same Question without Chat History is Not Accurate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "mk4eZ5SoeoNK",
        "outputId": "7200989e-92da-4be5-c65a-2a10c7406dbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The text does not mention the \"8b model\". It does mention quantizing the 405B model from 16-bit (BF16) to 8-bit (FP8) numerics, but it does not mention an \"8b model\" specifically."
          },
          "metadata": {}
        }
      ],
      "source": [
        "user_query = \"what about the 8b model?\"\n",
        "llm_output = chain.invoke({\"question\": user_query, \"chat_history\": []})\n",
        "md(llm_output['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNZq5ZGbeoNK"
      },
      "source": [
        "Without chat history, the model appears to just retrieve passages that approximate the semantic meaning of the word 'model' contained in the user question, but is not able to retrieve information about the context length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "6RFzz8qaeoNK",
        "outputId": "74ccd224-2875-4061-bdd1-c73907398cfb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Introducing Llama 3.1: Our most capable models to date"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "this blog post.)While this is our biggest model yet, we believe there’s still plenty of new ground to explore in the future, including more device-friendly sizes, additional modalities, and more investment at the agent platform layer.As always, we look forward to seeing all the amazing products and experiences the community will build with these models.This work was supported by our partners across the AI community. We’d like to thank and acknowledge (in alphabetical order): Accenture, Amazon"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "parameter model to improve the post-training quality of our smaller models.To support large-scale production inference for a model at the scale of the 405B, we quantized our models from 16-bit (BF16) to 8-bit (FP8) numerics, effectively lowering the compute requirements needed and allowing the model to run within a single server node.Instruction and chat fine-tuningWith Llama 3.1 405B, we strove to improve the helpfulness, quality, and detailed instruction-following capability of the model in"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "translation. With the release of the 405B model, we’re poised to supercharge innovation—with unprecedented opportunities for growth and exploration. We believe the latest generation of Llama will ignite new applications and modeling paradigms, including synthetic data generation to enable the improvement and training of smaller models, as well as model distillation—a capability that has never been achieved at this scale in open source.As part of this latest release, we’re introducing upgraded"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for doc in llm_output['source_documents']:\n",
        "    md(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFMp8hFEeoNL"
      },
      "source": [
        "### 4. Model Hallucination without RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0GNOH-JeoNL"
      },
      "source": [
        "Note that without RAG, the model generates an incorrect response, and that the user can not verify the information since the sources are not available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "XErAxQGFeoNL",
        "outputId": "8737423e-6487-4126-dab5-5ebc6aed2674"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "According to the official documentation, the context length in LLaMA 3.1 405B is 2048 tokens.\n\nIn other words, the model can process and respond to input sequences of up to 2048 tokens (or characters) in length."
          },
          "metadata": {}
        }
      ],
      "source": [
        "result = llm.invoke(\"how long is the context length in Llama 3.1 405B?\")\n",
        "md(result.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}